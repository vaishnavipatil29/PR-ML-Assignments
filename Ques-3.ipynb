{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold(n,df):\n",
    "    seed(1)\n",
    "    index1=cross_validation_split(df[0],5) \n",
    "    index1=pd.DataFrame(data=index1)\n",
    "    index1=index1.T\n",
    "    S=np.array([0,1,2,3,4]) # Number of folds\n",
    "    #testing set\n",
    "    X0_list=list()\n",
    "    X1_list=list()\n",
    "    X2_list=list()\n",
    "    Y_list=list()\n",
    "    list_df0 = list(df[0])\n",
    "    list_df1 = list(df[1])\n",
    "    list_df2 = list(df[2])\n",
    "    list_y = list(df[3])\n",
    "    for i in index1[5-n]:\n",
    "        X0_list.append(list_df0[i])\n",
    "        X1_list.append(list_df1[i])\n",
    "        X2_list.append(list_df2[i])\n",
    "        Y_list.append(list_y[i])\n",
    "    \n",
    "    x0_test=np.array(X0_list)\n",
    "    x1_test=np.array(X1_list)\n",
    "    x2_test=np.array(X2_list)\n",
    "    y_test=np.array(Y_list)\n",
    "   \n",
    "    S=np.delete(S, 5-n)\n",
    "    # training set\n",
    "    X0_list=list()\n",
    "    X1_list=list()\n",
    "    X2_list=list()\n",
    "    Y_list=list()\n",
    "    list_df0 = list(df[0])\n",
    "    list_df1 = list(df[1])\n",
    "    list_df2 = list(df[2])\n",
    "    list_y = list(df[3])\n",
    "    for j in S:\n",
    "        for i in index1[j]:\n",
    "            X0_list.append(list_df0[i])\n",
    "            X1_list.append(list_df1[i])\n",
    "            X2_list.append(list_df2[i])\n",
    "            Y_list.append(list_y[i])\n",
    "    x0_train=np.array(X0_list)\n",
    "    x1_train=np.array(X1_list)\n",
    "    x2_train=np.array(X2_list)\n",
    "    y_train=np.array(Y_list )\n",
    "    \n",
    "    \n",
    "    y_train=y_train.reshape(388,1)\n",
    "    x_train=np.vstack((x0_train,x1_train,x2_train))\n",
    "    x_test=np.vstack((x0_test,x1_test,x2_test))   \n",
    "    x_mean = x_train.T.mean(axis=0)\n",
    "    x_stdev = x_train.T.std(axis=0)\n",
    "    x_train_norm = (x_train.T - x_mean.T)/(x_stdev.T)\n",
    "    x_train_norm_aug = np.c_[np.ones((x_train_norm.shape[0], 1)), x_train_norm]\n",
    "\n",
    "    x_test_norm = (x_test.T - x_mean.T)/(x_stdev.T)\n",
    "    x_test_norm_aug = np.c_[np.ones((x_test_norm.shape[0], 1)), x_test_norm]\n",
    "    \n",
    "    return x_train,y_train,x_test,y_test,x_train_norm,x_train_norm_aug,x_test_norm,x_test_norm_aug,x0_train,x1_train,x2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lin_Reg_Mat_inv(x_train,y_train,x_test,y_test,x_train_norm,x_train_norm_aug,x_test_norm,x_test_norm_aug,x0_train,x1_train,x2_train):\n",
    "    print(\"Linear Regression: Matrix Inversion\")\n",
    "    reg = regression()\n",
    "    w_pred_mat_inv = reg.mat_inv(y_train,x_train_norm_aug.T)\n",
    "    print(\"Parameters\")\n",
    "    print(w_pred_mat_inv)\n",
    "\n",
    "    Ypred = np.dot(x_train_norm_aug, w_pred_mat_inv)\n",
    "\n",
    "    train_error = reg.error(w_pred_mat_inv, y_train, x_train_norm_aug.T)/((np.max(y_train)-np.mean(y_train))**2)\n",
    "\n",
    "    test_error = reg.error(w_pred_mat_inv, y_test, x_test_norm_aug.T)/((np.max(y_test)-np.mean(y_test))**2)\n",
    "\n",
    "    print('Normalized training error = ', train_error)\n",
    "    print('Normalized testing error = ', test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Gradient Descent Method\n",
    "def Lin_Reg_Grad_Descent(x_train,y_train,x_test,y_test,x_train_norm,x_train_norm_aug,x_test_norm,x_test_norm_aug,x0_train,x1_train,x2_train):\n",
    "    print(\"Linear Regression: Gradient Descent Method \")\n",
    "    lr = 0.01\n",
    "    reg = regression()\n",
    "    w_pred,err = reg.Regression_grad_des(x_train_norm_aug.T,y_train,lr)\n",
    "    print(\"Parameters:\")\n",
    "    print(w_pred)\n",
    "    train_error = reg.error(w_pred, y_train, x_train_norm_aug.T)/((np.max(y_train)-np.mean(y_train))**2)\n",
    "    test_error = reg.error(w_pred, y_test, x_test_norm_aug.T)/((np.max(y_test)-np.mean(y_test))**2)\n",
    "\n",
    "    print('Normalized training error = ', train_error)\n",
    "    print('Normalized testing error = ', test_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression\n",
      "1  - Fold\n",
      "Linear Regression: Matrix Inversion\n",
      "Parameters\n",
      "[[ 4.57675515]\n",
      " [ 0.64809252]\n",
      " [-0.77313948]\n",
      " [-0.36694189]]\n",
      "Normalized training error =  0.023521636606100592\n",
      "Normalized testing error =  0.23600446634498648\n",
      "2  - Fold\n",
      "Linear Regression: Matrix Inversion\n",
      "Parameters\n",
      "[[ 4.61031186]\n",
      " [ 0.55917371]\n",
      " [-0.82969551]\n",
      " [-0.39831487]]\n",
      "Normalized training error =  0.023815931619799544\n",
      "Normalized testing error =  0.16939340407906747\n",
      "3  - Fold\n",
      "Linear Regression: Matrix Inversion\n",
      "Parameters\n",
      "[[ 4.63071649]\n",
      " [ 0.55510568]\n",
      " [-0.80921835]\n",
      " [-0.38249743]]\n",
      "Normalized training error =  0.025217820095614775\n",
      "Normalized testing error =  0.16555079307156453\n",
      "4  - Fold\n",
      "Linear Regression: Matrix Inversion\n",
      "Parameters\n",
      "[[ 4.6095    ]\n",
      " [ 0.60173217]\n",
      " [-0.79690186]\n",
      " [-0.33106879]]\n",
      "Normalized training error =  0.025962673557047053\n",
      "Normalized testing error =  0.1815224276826107\n",
      "5  - Fold\n",
      "Linear Regression: Matrix Inversion\n",
      "Parameters\n",
      "[[ 4.56814948]\n",
      " [ 0.58077889]\n",
      " [-0.7874266 ]\n",
      " [-0.30916919]]\n",
      "Normalized training error =  0.02861425386763583\n",
      "Normalized testing error =  0.1931780835639759\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear Regression\")\n",
    "for f in range(1,6):\n",
    "    print(f,\" - Fold\")\n",
    "    x_train,y_train,x_test,y_test,x_train_norm,x_train_norm_aug,x_test_norm,x_test_norm_aug,x0_train,x1_train,x2_train=fold(f,df)\n",
    "    Lin_Reg_Mat_inv(x_train,y_train,x_test,y_test,x_train_norm,x_train_norm_aug,x_test_norm,x_test_norm_aug,x0_train,x1_train,x2_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression\n",
      "1  - Fold\n",
      "Linear Regression: Gradient Descent Method \n",
      "Parameters:\n",
      "[[ 4.57675515]\n",
      " [ 0.64816528]\n",
      " [-0.77306518]\n",
      " [-0.36694464]]\n",
      "Normalized training error =  0.02352163673804318\n",
      "Normalized testing error =  0.23600534716132615\n",
      "2  - Fold\n",
      "Linear Regression: Gradient Descent Method \n",
      "Parameters:\n",
      "[[ 4.61031185]\n",
      " [ 0.5593609 ]\n",
      " [-0.82949339]\n",
      " [-0.39833699]]\n",
      "Normalized training error =  0.02381593263980761\n",
      "Normalized testing error =  0.16938960924135713\n",
      "3  - Fold\n",
      "Linear Regression: Gradient Descent Method \n",
      "Parameters:\n",
      "[[ 4.63071649]\n",
      " [ 0.55538884]\n",
      " [-0.80892082]\n",
      " [-0.38251906]]\n",
      "Normalized training error =  0.025217822293481698\n",
      "Normalized testing error =  0.165546036183403\n",
      "4  - Fold\n",
      "Linear Regression: Gradient Descent Method \n",
      "Parameters:\n",
      "[[ 4.60949999]\n",
      " [ 0.60188276]\n",
      " [-0.79674205]\n",
      " [-0.33108419]]\n",
      "Normalized training error =  0.02596267420688555\n",
      "Normalized testing error =  0.18151836901365115\n",
      "5  - Fold\n",
      "Linear Regression: Gradient Descent Method \n",
      "Parameters:\n",
      "[[ 4.56814948]\n",
      " [ 0.58101272]\n",
      " [-0.78716669]\n",
      " [-0.30921286]]\n",
      "Normalized training error =  0.02861425564656055\n",
      "Normalized testing error =  0.19318362097359576\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear Regression\")\n",
    "for f in range(1,6):\n",
    "    print(f,\" - Fold\")\n",
    "    x_train,y_train,x_test,y_test,x_train_norm,x_train_norm_aug,x_test_norm,x_test_norm_aug,x0_train,x1_train,x2_train=fold(f,df)\n",
    "    Lin_Reg_Grad_Descent(x_train,y_train,x_test,y_test,x_train_norm,x_train_norm_aug,x_test_norm,x_test_norm_aug,x0_train,x1_train,x2_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transform(X,degree): \n",
    "    X_new=np.ones((X.shape[0],1))\n",
    "    for i in range(1,degree+1): \n",
    "        X_new = np.c_[X_new,X**i]\n",
    "    return X_new.T\n",
    "\n",
    "def Poly_Mat_inv(x_train,y_train,x_test,y_test,x_train_norm,x_train_norm_aug,x_test_norm,x_test_norm_aug,x0_train,x1_train,x2_train,deg):\n",
    "    print(\"Polynomial Regression-Matrix Inversion\")\n",
    "    reg=regression()\n",
    "    for degree in range(0,deg+1):\n",
    "        print(\"degree: \",degree)\n",
    "        X_0=data_transform(x_train_norm,degree)\n",
    "        w_mat = reg.mat_inv(y_train, X_0)\n",
    "        y_pred=X_0.T @ w_mat\n",
    "        train_error = reg.error(w_mat, y_train, X_0)/((np.max(y_train)-np.mean(y_train))**2)\n",
    "        X_0=data_transform(x_test_norm,degree)\n",
    "        test_error = reg.error(w_mat, y_test, X_0)/((np.max(y_test)-np.mean(y_test))**2)\n",
    "\n",
    "        print('Normalized training error = ', train_error)\n",
    "        print('Normalized testing error = ', test_error)\n",
    "        \n",
    "# By Gradient Descent Method\n",
    "def Poly_Grad_Descent(x_train,y_train,x_test,y_test,x_train_norm,x_train_norm_aug,x_test_norm,x_test_norm_aug,x0_train,x1_train,x2_train,deg):\n",
    "    print(\"Polynomial Regression-Gradient Descent\")\n",
    "    reg=regression()\n",
    "    for degree in range(0,deg+1):\n",
    "        print(\"degree: \",degree)\n",
    "        X_0=data_transform(x_train_norm,degree)\n",
    "        lr = 0.01\n",
    "        w_pred,err = reg.Regression_grad_des(X_0,y_train,lr)\n",
    "        train_error = reg.error(w_pred, y_train, X_0)/((np.max(y_train)-np.mean(y_train))**2)\n",
    "        X_0=data_transform(x_test_norm,degree)\n",
    "        test_error = reg.error(w_pred, y_test, X_0)/((np.max(y_test)-np.mean(y_test))**2)\n",
    "\n",
    "        print('Normalized training error = ', train_error)\n",
    "        print('Normalized testing error = ', test_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Regression\n",
      "1  - Fold\n",
      "Polynomial Regression-Matrix Inversion\n",
      "degree:  0\n",
      "Normalized training error =  0.09167857605784512\n",
      "Normalized testing error =  0.13523788913257284\n",
      "degree:  1\n",
      "Normalized training error =  0.023521636606100592\n",
      "Normalized testing error =  0.23600446634498648\n",
      "degree:  2\n",
      "Normalized training error =  0.015960124846897957\n",
      "Normalized testing error =  0.24254498727741475\n",
      "degree:  3\n",
      "Normalized training error =  0.01580576518892422\n",
      "Normalized testing error =  0.2411732553235779\n",
      "degree:  4\n",
      "Normalized training error =  0.014673412314078616\n",
      "Normalized testing error =  0.24387502964651955\n",
      "2  - Fold\n",
      "Polynomial Regression-Matrix Inversion\n",
      "degree:  0\n",
      "Normalized training error =  0.09267182404591212\n",
      "Normalized testing error =  0.09372812437458397\n",
      "degree:  1\n",
      "Normalized training error =  0.023815931619799544\n",
      "Normalized testing error =  0.16939340407906747\n",
      "degree:  2\n",
      "Normalized training error =  0.0159495296391078\n",
      "Normalized testing error =  0.16725702345461468\n",
      "degree:  3\n",
      "Normalized training error =  0.015717973729820974\n",
      "Normalized testing error =  0.1667925838720956\n",
      "degree:  4\n",
      "Normalized training error =  0.014429098283268443\n",
      "Normalized testing error =  0.16937590564106975\n",
      "3  - Fold\n",
      "Polynomial Regression-Matrix Inversion\n",
      "degree:  0\n",
      "Normalized training error =  0.09224089720379952\n",
      "Normalized testing error =  0.1013724257701077\n",
      "degree:  1\n",
      "Normalized training error =  0.025217820095614775\n",
      "Normalized testing error =  0.16555079307156453\n",
      "degree:  2\n",
      "Normalized training error =  0.01596299394781603\n",
      "Normalized testing error =  0.18058496773797442\n",
      "degree:  3\n",
      "Normalized training error =  0.015428410991253473\n",
      "Normalized testing error =  0.1848977666160313\n",
      "degree:  4\n",
      "Normalized training error =  0.014483156549235755\n",
      "Normalized testing error =  0.18067130984687013\n",
      "4  - Fold\n",
      "Polynomial Regression-Matrix Inversion\n",
      "degree:  0\n",
      "Normalized training error =  0.08995252213172565\n",
      "Normalized testing error =  0.10598898715323272\n",
      "degree:  1\n",
      "Normalized training error =  0.025962673557047053\n",
      "Normalized testing error =  0.1815224276826107\n",
      "degree:  2\n",
      "Normalized training error =  0.015611039597029365\n",
      "Normalized testing error =  0.18617155791959233\n",
      "degree:  3\n",
      "Normalized training error =  0.015307613998153889\n",
      "Normalized testing error =  0.18342267868731305\n",
      "degree:  4\n",
      "Normalized training error =  0.014439472653295326\n",
      "Normalized testing error =  0.1891663812910955\n",
      "5  - Fold\n",
      "Polynomial Regression-Matrix Inversion\n",
      "degree:  0\n",
      "Normalized training error =  0.09827920155540042\n",
      "Normalized testing error =  0.11130803465038014\n",
      "degree:  1\n",
      "Normalized training error =  0.02861425386763583\n",
      "Normalized testing error =  0.1931780835639759\n",
      "degree:  2\n",
      "Normalized training error =  0.017892102604210952\n",
      "Normalized testing error =  0.2211805270296784\n",
      "degree:  3\n",
      "Normalized training error =  0.017103006019772854\n",
      "Normalized testing error =  0.22723231781537417\n",
      "degree:  4\n",
      "Normalized training error =  0.015870365769582877\n",
      "Normalized testing error =  0.22537500139288832\n"
     ]
    }
   ],
   "source": [
    "print(\"Polynomial Regression\")\n",
    "for f in range(1,6):\n",
    "    print(f,\" - Fold\")\n",
    "    x_train,y_train,x_test,y_test,x_train_norm,x_train_norm_aug,x_test_norm,x_test_norm_aug,x0_train,x1_train,x2_train=fold(f,df)\n",
    "    Poly_Mat_inv(x_train,y_train,x_test,y_test,x_train_norm,x_train_norm_aug,x_test_norm,x_test_norm_aug,x0_train,x1_train,x2_train,4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Regression\n",
      "1  - Fold\n",
      "Polynomial Regression-Gradient Descent\n",
      "degree:  0\n",
      "Normalized training error =  0.09167857605784512\n",
      "Normalized testing error =  0.13523788920331944\n",
      "degree:  1\n",
      "Normalized training error =  0.02352164137808925\n",
      "Normalized testing error =  0.23600978238029985\n",
      "degree:  2\n",
      "Normalized training error =  0.01598123353601916\n",
      "Normalized testing error =  0.24334597557944815\n",
      "degree:  3\n",
      "Normalized training error =  0.015884963394764792\n",
      "Normalized testing error =  0.2417221598679061\n",
      "2  - Fold\n",
      "Polynomial Regression-Gradient Descent\n",
      "degree:  0\n",
      "Normalized training error =  0.09267182404591212\n",
      "Normalized testing error =  0.09372812434339489\n",
      "degree:  1\n",
      "Normalized training error =  0.0238159320465563\n",
      "Normalized testing error =  0.16939095809133023\n",
      "degree:  2\n",
      "Normalized training error =  0.015964013319621394\n",
      "Normalized testing error =  0.16723954639433994\n",
      "degree:  3\n",
      "Normalized training error =  0.01582012730588988\n",
      "Normalized testing error =  0.16733747930752468\n",
      "3  - Fold\n",
      "Polynomial Regression-Gradient Descent\n",
      "degree:  0\n",
      "Normalized training error =  0.09224089720379952\n",
      "Normalized testing error =  0.10137242569057124\n",
      "degree:  1\n",
      "Normalized training error =  0.025217820815505165\n",
      "Normalized testing error =  0.1655480692671913\n",
      "degree:  2\n",
      "Normalized training error =  0.01597504939452337\n",
      "Normalized testing error =  0.1805690267337068\n",
      "degree:  3\n",
      "Normalized training error =  0.015555786715159932\n",
      "Normalized testing error =  0.18522911321278368\n",
      "4  - Fold\n",
      "Polynomial Regression-Gradient Descent\n",
      "degree:  0\n",
      "Normalized training error =  0.08995252213172565\n",
      "Normalized testing error =  0.10598898712708069\n",
      "degree:  1\n",
      "Normalized training error =  0.025962674061805774\n",
      "Normalized testing error =  0.1815188532019939\n",
      "degree:  2\n",
      "Normalized training error =  0.015622365060817385\n",
      "Normalized testing error =  0.18686962997165235\n",
      "degree:  3\n",
      "Normalized training error =  0.015403678155228155\n",
      "Normalized testing error =  0.18420403698704657\n",
      "5  - Fold\n",
      "Polynomial Regression-Gradient Descent\n",
      "degree:  0\n",
      "Normalized training error =  0.09827920155540042\n",
      "Normalized testing error =  0.1113080347227856\n",
      "degree:  1\n",
      "Normalized training error =  0.02861425589120043\n",
      "Normalized testing error =  0.19318398995292935\n",
      "degree:  2\n",
      "Normalized training error =  0.017920222980031188\n",
      "Normalized testing error =  0.22175629696170812\n",
      "degree:  3\n",
      "Normalized training error =  0.017183395081068784\n",
      "Normalized testing error =  0.22874450130127721\n"
     ]
    }
   ],
   "source": [
    "print(\"Polynomial Regression\")\n",
    "for f in range(1,6):\n",
    "    print(f,\" - Fold\")\n",
    "    x_train,y_train,x_test,y_test,x_train_norm,x_train_norm_aug,x_test_norm,x_test_norm_aug,x0_train,x1_train,x2_train=fold(f,df)\n",
    "    Poly_Grad_Descent(x_train,y_train,x_test,y_test,x_train_norm,x_train_norm_aug,x_test_norm,x_test_norm_aug,x0_train,x1_train,x2_train,3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Fit for Feature 1-2-3 is found to be polynomial regression of degree 2\n",
      "Parameters\n",
      "[ 4.2504794   0.50108123 -0.94310413 -0.3593963   0.22406877  0.11226913\n",
      " -0.04439224]\n",
      "Normalized training error =  0.015600557307775879\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Fit for Feature 1-2-3 is found to be polynomial regression of degree 2\")\n",
    "X0=df[0]\n",
    "X1=df[1]\n",
    "X3=df[2]\n",
    "Y=df[3]\n",
    "x=np.vstack((X0,X1,X3))\n",
    "x_mean = x.T.mean(axis=0)\n",
    "x_stdev = x.T.std(axis=0)\n",
    "x_norm = (x.T - x_mean.T)/(x_stdev.T)\n",
    "x_norm_aug = np.c_[np.ones((x_norm.shape[0], 1)), x_norm]\n",
    "reg = regression()\n",
    "X = data_transform(x_norm,2)\n",
    "#print(\"Matrix Inversion\")\n",
    "w_mat = reg.mat_inv(Y, X)\n",
    "print(\"Parameters\")\n",
    "print(w_mat)\n",
    "y_pred=X.T @ w_mat\n",
    "train_error = reg.error(w_mat, Y, X)/((np.max(Y)-np.mean(Y))**2)\n",
    "print('Normalized training error = ', train_error)\n",
    "X = data_transform(x_norm,2)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
